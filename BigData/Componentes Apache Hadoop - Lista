## ECOSSISTEMA HADOOP  (PRINCIPAIS FERRAMENTAS) ##


CLUSTER = UM SERVIDOR.


HFDS -> É UM SISTEMA DE ARQUIVOS DISTRIBUIDOS
	(ELE PEGA O ARQUIVO QUE INSERIMOS NELE DIVIDE PELOS DATANODES DISPONIVEIS EM PARTES DE 128MB, DIVIDE EM 3 CLUSTERS DIFERENTES E DESTA FORMA O PROCESSAMENTO FICA MAIS RÁPIDO. CASO UM CLUSTER DÊ PROBLEMA ELE REPLICA O DADO EM OUTRO LUGAR E DESTA FORMA ELE É TOLERANTE HÁ FALHAS E TAMBÉM CONSEGUE ARMAZENAR GRANDES QUANTIDADES DE REGISTROS.)


MAP REDUCE -> ALGORITIMO QUE FAZ ACESSO AO HDFS E FAZ O PROCESSAMENTO DOS ARQUIVOS. 
	(ELE PEGA O PROGRAMA E MANDA PARA ONDE ESTÃO OS BLOCOS DO ARQUIVO A SER PROCESSADO, MAPEIA EM QUAIS CLUSTERS ESTÃO AS PARTES DO ARQUIVO QUE SERÁ NECESSÁRIO PROCESSAR, MANDA CADA CLUSTER FAZER O PROCESSAMENTO DESSE ARQUIVO E NO FINAL ELE REUNE O PROCESSAMENTO DE TODAS AS PARTES DO ARQUIVO.
	ISTO FAZ TAMBÉM COM QUE CONSIGAMOS TER O PROCESSAMENTO PARALELO)


HIVE -> SIMULA OS COMANDOS SQL PARA ACESSAR ARQUIVOS ARMAZENADOS NO HDFS.
	(ELE RECEBE COMO ENTRADA CONSULTAS SQL E TRANSCREVE ESSAS CONSULTAS EM APLICAÇÕES MAPREDUCE E ASSIM SÃO ENVIADAS PARA O CLUSTER PARA FAZER O PROCESSAMENTO.
	ELE NÃO É UM BANCO RELACIONAL, ELE É UMA FERRAMENTO QUE FAZ O PROCESSAMENTO DISTRIBUIDO DE UM GRANDE VOLUME DE DADOS.

	NO HIVE PODEMOS CRIAR APENAS 2 TIPOS DE TABELAS:

		TABELAS GERENCIADAS: ONDE TEM UM LUGAR ESPECIFICO DO HDFS QUE ELE GUARDA A TABELA/BANCO DE DADOS.

		TABELAS EXTERNAS: CRIAR UMA TABELA APONTANDO PARA O DIRETÓRIO ONDE ESTÃO OS MEUS DADOS.


	TEMOS DUAS FORMAS DE FAZER A CARGA DOS DADOS NO HIVE:

		1. UTILIZAR OS COMANDOS DO HDFS PARA LEVAR OS DADOS DE UMA ORIGEM PARA DENTRO DO HDFS E DEPOIS A CRIAÇAO DA TABELA EXTERNA PEGANDO ESSES DADOS.

		2. USAR O COMANDO "LOAD" QUE LEVA OS DADOS DIRETO DA ORIGEM PARA DENTRO DO HIVE.
	)

YARN -> FERRAMENTA QUE GERENCIA O CLUSTER DO HDFS.

HBASE -> BASE DE DADOS NOSQL NO FORMATO COLUNAR.

R CONNECTIONS -> PROGRAMA UTILIZADO PARA ESTATISTICA.

MAHOUT (MACHINE LEARNING) -> CONJUNTO DE ALGORITIMOS PARA APRENDIZADO DE MÁQUINA.

PIG -> PARCEIRO IDEAL DO HDFS, FAZ SCRIPTS DE ACESSO AOS DADOS ARMAZENADOS NO HDFS. 

OOZIE -> FAZ A ORGANIZAÇÃO DE DADOS CHEGAM EM TODOS DIFERENTES.

ZOOKEEPER -> FAZ A COORDENAÇÃO DOS CLUSTERS.

FLUME -> FERRAMENTA PARA INGESTÃO DE DADOS DENTRO DO CLUSTER HADOOP.

SQOOP -> FAZ A CONEXÃO COM BASES RELACIONAIS PARA FAZER INGESTÃO DE VALORES.

SPARK -> FRAMEWORK PARA PROCESSAMENTO DISTRIBUIDO.

KAFKA -> SERVIÇO DE ENTREGA E RECEBIMENTO DE MENSAGEM, ELE TE GARANTE A ENTREGA PARA OUTRO SERVIÇO QUE VAI RECEBER ESSA INFORMAÇÃO.

AMBARI -> PERMITE QUE OS ADMINISTRADORES DE SISTEMA PROVISIONEM, GERENCIEM E MONITOREM UM CLUSTER HADOOP E TAMBÉM INTEGREM O HADOOP À INFRAESTRUTURA CORPORATIVA EXISTENTE.
	(ELE POR MEIO DE UMA INTERFACE WEB PERMITE A ADMINISTRAÇÃO DE UM CLUSTER, VOCE PODE CONTROLAR OS CLUSTER DO ECOSSITEMA HADOOP.
	FAZ INTEGRAÇÃO COM FERRAMENTAS DE AUTENTICAÇÃO)



## CURIOSIDADES ##

HIVE E PIG DEPENDEM DO MAP REDUCE

IMPALA NÃO UTILIZA O MAP REDUCE, DESTA FORMA ELE TEM MAIS PERFORMANCE NA PARTE DE CONSULTA DOS DADOS. (IMPALA UTILIZA O PLSQL)

	- SEMPRE QUE ACESSAR O HIVE DEVEMOS RODAR O COMANDO "INVALIDATE METADATA"



###############################

TAREFA PARA FAZER AMANHÃ:

LISTAR OS ARQUIVOS NECESSÁRIOS DO CARTOLA
INSERIR ESSES ARQUIVOS EM UMA PASTA COM O NOME LEGAL E BEM SEPARADOS NO HDFS
LEVAR ESSE DADOS PARA O HIVE
CRIAR TABELAS NO IMPALA
CRIAR QUERIES PARA EXTRAIR OS INDICADORES NECESSÁRIOS
